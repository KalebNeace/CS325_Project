import requests
from bs4 import BeautifulSoup
import time

def scrape_comments(url):
    comments = []
    while url:
        try:
            print(f"Scraping {url}")
            response = requests.get(url, timeout=10)  # Timeout if takes to long
            response.raise_for_status()  # Check for HTTP errors
            soup = BeautifulSoup(response.text, 'html.parser')

            # Debug: Print the HTML structure
            # print(soup.prettify())  # Uncomment to see the full HTML structure

            reviews = soup.select('.fdbk-container__details__top') # Find this class thats contains the comments 
            for review in reviews:
                comments.append(review.get_text(strip=True))


            # Find the link for the next page
            next_page = soup.select_one('.pagination__items a') # Find class that brings you to the next page of comments
            if next_page:
                print("Found next page:", next_page['href'])
                url = next_page['href']
            else:
                print("No next page found.")
                url = None

            # Delay to avoid overwhelming the server
            time.sleep(1)  # Adjust the sleep duration as needed
        except Exception as e:
            print(f"Error fetching {url}: {e}")
            break
    print(f"Collected {len(comments)} comments.") # Number of comments collected
    return comments

# Creates the files and outputs the comments to files
def save_comments(version, comments):
    with open(f'{version}_comments.txt', 'w', encoding='utf-8') as file: # Create a new file to output the comments into
        for comment in comments:
            file.write('-' + comment + '\n' + '\n') # Write the comments to the new file

# Read URLs from the file
def read_urls_from_file(filename):
    with open(filename, 'r', encoding='utf-8') as file:
        return [line.strip() for line in file if line.strip()]



urls = read_urls_from_file('Links.txt')
version = ["s22","s21","s20","s10"] # For naming purposes
count = 0

for url in urls:
    comments = scrape_comments(url)
    save_comments(version[count], comments)
    count += 1