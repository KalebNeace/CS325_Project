from llama_cpp import Llama

# Initialize the Llama model with the following specifications
llm = Llama(
    model_path="./Phi-3-mini-4k-instruct-q4.gguf",  # path to GGUF file
    n_ctx=4096,  # The max sequence length to use
    n_threads=8,  # The number of CPU threads to use
    n_gpu_layers=35,  # The number of layers to offload to GPU
)

# Read the prompt from a file
with open('Input.txt', 'r') as input_file:
    prompt = input_file.read().strip()

output = llm( # llm calls the Llama model to generate a response based on the formatted input
    f"<|user|>\n{prompt}<|end|>\n<|assistant|>", # 'f' f-string (formatted string literal) 
    max_tokens=256,  # Generate up to 256 tokens
    stop=["<|end|>"], # the stopping parameter, when the AI should stop
    echo=True,  # Whether to include the prompt in the response
)

# Write the output to a file
with open('Output.txt', 'w') as output_file:
    output_file.write(output['choices'][0]['text'])
                            # choices -  a list of possible responses generated by the model
                            # 0 - index accesses the first response in the choices
                            # text - contains the actual generated response (the assistant's reply) from the model

print("DONE: view response in Output.txt")